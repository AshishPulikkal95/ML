{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries  \n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, Activation\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adamax\n",
    "import numpy as np\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "\n",
    "# data from dataset \n",
    "X_train = genfromtxt('train_data.csv', delimiter=',')\n",
    "y_train = genfromtxt('train_target.csv', delimiter=',')\n",
    "X_test = genfromtxt('test_data.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashish\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), strides=(2, 2), data_format=\"channels_last\")`\n",
      "C:\\Users\\Ashish\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), strides=(2, 2), data_format=\"channels_last\")`\n",
      "C:\\Users\\Ashish\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), strides=(2, 2), data_format=\"channels_last\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10352 samples, validate on 2588 samples\n",
      "Epoch 1/20\n",
      "10352/10352 [==============================] - 1108s 107ms/step - loss: 2.7068 - acc: 0.4452 - val_loss: 1.5950 - val_acc: 0.5703\n",
      "Epoch 2/20\n",
      "10352/10352 [==============================] - 1318s 127ms/step - loss: 1.6053 - acc: 0.5299 - val_loss: 0.9137 - val_acc: 0.6159\n",
      "Epoch 3/20\n",
      "10352/10352 [==============================] - 1068s 103ms/step - loss: 1.2539 - acc: 0.5862 - val_loss: 0.8901 - val_acc: 0.6221\n",
      "Epoch 4/20\n",
      "10352/10352 [==============================] - 1070s 103ms/step - loss: 1.1043 - acc: 0.6242 - val_loss: 0.8163 - val_acc: 0.6406\n",
      "Epoch 5/20\n",
      "10352/10352 [==============================] - 1063s 103ms/step - loss: 1.0051 - acc: 0.6492 - val_loss: 0.7676 - val_acc: 0.6754\n",
      "Epoch 6/20\n",
      "10352/10352 [==============================] - 1068s 103ms/step - loss: 0.9399 - acc: 0.6753 - val_loss: 0.9266 - val_acc: 0.6716\n",
      "Epoch 7/20\n",
      "10352/10352 [==============================] - 1070s 103ms/step - loss: 0.8905 - acc: 0.6912 - val_loss: 0.8068 - val_acc: 0.6662\n",
      "Epoch 8/20\n",
      "10352/10352 [==============================] - 1058s 102ms/step - loss: 0.8112 - acc: 0.7060 - val_loss: 0.7642 - val_acc: 0.6835\n",
      "Epoch 9/20\n",
      "10352/10352 [==============================] - 1070s 103ms/step - loss: 0.7357 - acc: 0.7356 - val_loss: 0.7730 - val_acc: 0.6832\n",
      "Epoch 10/20\n",
      "10352/10352 [==============================] - 1058s 102ms/step - loss: 0.7177 - acc: 0.7387 - val_loss: 0.7054 - val_acc: 0.6824\n",
      "Epoch 11/20\n",
      "10352/10352 [==============================] - 1096s 106ms/step - loss: 0.6673 - acc: 0.7480 - val_loss: 0.7501 - val_acc: 0.6978\n",
      "Epoch 12/20\n",
      "10352/10352 [==============================] - 1093s 106ms/step - loss: 0.5672 - acc: 0.7837 - val_loss: 0.7679 - val_acc: 0.7152\n",
      "Epoch 13/20\n",
      "10352/10352 [==============================] - 1079s 104ms/step - loss: 0.4996 - acc: 0.8061 - val_loss: 0.7660 - val_acc: 0.7044\n",
      "Epoch 14/20\n",
      "10352/10352 [==============================] - 1096s 106ms/step - loss: 0.4481 - acc: 0.8275 - val_loss: 0.7599 - val_acc: 0.7318\n",
      "Epoch 15/20\n",
      "10352/10352 [==============================] - 1519s 147ms/step - loss: 0.4148 - acc: 0.8421 - val_loss: 0.8939 - val_acc: 0.6940\n",
      "Epoch 16/20\n",
      "10352/10352 [==============================] - 1065s 103ms/step - loss: 0.3967 - acc: 0.8471 - val_loss: 0.8868 - val_acc: 0.7160\n",
      "Epoch 17/20\n",
      "10352/10352 [==============================] - 1071s 103ms/step - loss: 0.3523 - acc: 0.8661 - val_loss: 0.8724 - val_acc: 0.7156\n",
      "Epoch 18/20\n",
      "10352/10352 [==============================] - 1074s 104ms/step - loss: 0.3186 - acc: 0.8814 - val_loss: 0.9815 - val_acc: 0.7067\n",
      "Epoch 19/20\n",
      "10352/10352 [==============================] - 1060s 102ms/step - loss: 0.2940 - acc: 0.8904 - val_loss: 0.9872 - val_acc: 0.7411\n",
      "Epoch 20/20\n",
      "10352/10352 [==============================] - 1078s 104ms/step - loss: 0.2498 - acc: 0.9037 - val_loss: 0.9854 - val_acc: 0.7272\n"
     ]
    }
   ],
   "source": [
    "# We have to reshape the input to fit into the Tensorflow \n",
    "X_train = X_train.reshape(X_train.shape[0], 48, 48,1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 48, 48,1 )\n",
    "\n",
    "# Typecast the pixels of test and train\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Divide the data with the maximum color value i.e. 255\n",
    "X_train = X_train/ 255.0\n",
    "X_test = X_test/255.0\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, 3)\n",
    "\n",
    "#VGG model added batch normalization after convolutional layer.\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers of zeropadding, convolutional layer, and maxPooling (batch normalisation on each layer)\n",
    "model.add(ZeroPadding2D((1, 1), input_shape=(48,48,1)))\n",
    "model.add(Convolution2D(100, (3, 3), strides=(1, 1), padding =\"same\", activation='relu')) \n",
    "model.add (ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(100, (3, 3), strides=(1, 1), padding =\"same\", activation='relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides = (2, 2), dim_ordering=\"tf\"))\n",
    "model.add (ZeroPadding2D((1,1)))\n",
    "\n",
    "model.add(Convolution2D(256, (3, 3), strides=(1, 1), padding =\"same\", activation='relu')) \n",
    "model.add (ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), strides=(1, 1), padding =\"same\", activation='relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides = (2, 2), dim_ordering=\"tf\"))\n",
    "model.add (ZeroPadding2D((1,1)))\n",
    "\n",
    "model.add(Convolution2D(512, (3, 3), strides=(1, 1), padding =\"same\", activation='relu')) \n",
    "model.add (ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), strides=(1, 1), padding =\"same\", activation='relu')) \n",
    "model.add (ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), strides=(1, 1), padding =\"same\", activation='relu')) \n",
    "model.add (ZeroPadding2D((1,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides = (2, 2), dim_ordering=\"tf\"))\n",
    "\n",
    "# Finally add the fully connected layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu')) \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(512, activation='relu')) \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.8)) \n",
    "\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model with optimization\n",
    "adam = Adamax(lr=0.0015, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam , metrics=['accuracy'])\n",
    "\n",
    "# Train the model using train_data\n",
    "model.fit(X_train, Y_train, batch_size=128, epochs=100, validation_split=0.15) # batch = 100 split= 0.10\n",
    "\n",
    "# Predicting the results based on learning\n",
    "predictions = model.predict(X_test)\n",
    "#Convert prediction from probabilites in vectors ([.11,.22,.5]) to scalar integers ([0],[1],[2])\n",
    "output = predictions.argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the prediction csv file.....!!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "# Writes prediction to a csv file\n",
    "def write_csv(ar, file_name=\"Result\"):\n",
    "    with open(file_name+\".csv\", 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile,delimiter=',',quoting = csv.QUOTE_NONE)\n",
    "        row = ['Id','Category']\n",
    "        writer.writerow(row)\n",
    "        for i in range(ar.shape[0]):\n",
    "            row = [i,ar[i]]  \n",
    "            writer.writerow(row)\n",
    "            i+=1\n",
    "            \n",
    "#writing prediction into a csv file            \n",
    "write_csv(output)\n",
    "print(\"Successfully created the prediction csv file.....!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
